# Base Configuration for Weather Forecasting LoRA
# Following Schulman et al. (2025) methodology

# Model Configuration
model:
  base_model: "mistralai/Mistral-7B-v0.1"  # Alternative: "meta-llama/Meta-Llama-3-8B-Instruct"
  model_type: "causal_lm"
  device_map: "auto"
  torch_dtype: "float16"
  trust_remote_code: true

# LoRA Configuration (Schulman et al. recommendations)
lora:
  r: 32                    # Rank (16-64 range, 32 is sweet spot)
  alpha: 32               # Alpha scaling = 32 (per Schulman)
  dropout: 0.05           # Light dropout for stability
  bias: "none"            # No bias adaptation
  task_type: "CAUSAL_LM"
  
  # Target all linear layers (Schulman Sec. 2.2)
  target_modules:
    - "q_proj"      # Query projection
    - "k_proj"      # Key projection  
    - "v_proj"      # Value projection
    - "o_proj"      # Output projection
    - "gate_proj"   # Gate projection (MLP)
    - "up_proj"     # Up projection (MLP)
    - "down_proj"   # Down projection (MLP)

# Training Configuration
training:
  # Learning rate scaling (Schulman Sec. 3.1): LoRA LR ~10x FullFT LR
  learning_rate: 5e-5     # 10x higher than typical 5e-6 for full fine-tuning
  warmup_ratio: 0.05      # 5% warmup steps
  weight_decay: 0.01      # Regularization
  
  # Batch size (Schulman Sec. 4.2): moderate for stability
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 8  # Effective batch size = 32
  
  # Training duration
  num_train_epochs: 3
  max_steps: -1           # Use epochs instead
  eval_steps: 500
  save_steps: 1000
  logging_steps: 100
  
  # Optimization
  optim: "adamw_torch"
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8
  max_grad_norm: 1.0
  
  # Scheduler
  lr_scheduler_type: "cosine"
  
  # Mixed precision
  fp16: true
  dataloader_pin_memory: true

# Data Configuration  
data:
  max_length: 512         # Maximum sequence length
  truncation: true
  padding: "max_length"
  
  # Prompt template
  prompt_template: |
    Weather data for {location} on {datetime}:
    - Temperature (Â°C): {temperature}
    - Humidity (%): {humidity} 
    - Wind speed (km/h): {wind_speed}
    - Pressure (hPa): {pressure}
    - Precipitation probability: {precip_prob}
    
    Generate a forecast bulletin:
  
  # Target format
  target_template: |
    {forecast_text}

# Evaluation Configuration
evaluation:
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  evaluation_strategy: "steps"
  save_strategy: "steps"
  load_best_model_at_end: true

# Output Configuration
output:
  output_dir: "./models/weather-lora-sft"
  logging_dir: "./logs"
  run_name: "weather-lora-sft"
  report_to: ["tensorboard", "wandb"]  # Integrated W&B tracking

# Weights & Biases Configuration
wandb:
  # Project settings
  project: "weather-forecasting-lora"
  entity: ashioyajotham
  name: null                # Run name (auto-generated if null)
  group: "sft-experiments"  # Group related runs
  tags: 
    - "lora"
    - "weather-forecasting"
    - "schulman-2025"
  notes: "Weather forecasting with LoRA following Schulman et al. (2025)"
  
  # Logging configuration
  log_model: "checkpoint"   # Options: false, "checkpoint", "end"
  log_freq: 100             # Log metrics every N steps
  log_gradients: true       # Log gradient statistics
  log_parameters: true      # Log parameter statistics
  
  # What to track
  watch_model: true         # Track model architecture
  watch_freq: 1000          # How often to log gradients/parameters
  
  # Artifacts
  save_code: true           # Save code snapshot
  log_artifacts: true       # Save model checkpoints as artifacts
  artifact_type: "model"    # Artifact type for models
  
  # Evaluation tracking
  log_predictions: true     # Log sample predictions during eval
  num_predictions: 10       # Number of predictions to log
  
  # Custom metrics
  custom_metrics:
    - "bleu_score"
    - "rouge_1_f"
    - "rouge_2_f"
    - "rouge_l_f"
    - "temperature_mae"
    - "temperature_accuracy"
    - "wind_speed_mae"
    - "precipitation_accuracy"
  
  # Performance tracking
  log_system_metrics: true  # Track GPU/CPU/memory
  
  # Reinit settings
  reinit: true              # Allow multiple runs in same process
  resume: "allow"           # Resume runs if id matches
  
# Hardware Configuration
hardware:
  # Memory optimization
  gradient_checkpointing: true
  remove_unused_columns: false
  
  # Multi-GPU settings
  ddp_find_unused_parameters: false
  dataloader_num_workers: 4

# Seed for reproducibility
seed: 42