# Base Configuration for Weather Forecasting LoRA
# Following Schulman et al. (2025) methodology

# Model Configuration
model:
  base_model: "microsoft/Mistral-7B-v0.1"  # Alternative: "meta-llama/Meta-Llama-3-8B-Instruct"
  model_type: "causal_lm"
  device_map: "auto"
  torch_dtype: "float16"
  trust_remote_code: true

# LoRA Configuration (Schulman et al. recommendations)
lora:
  r: 32                    # Rank (16-64 range, 32 is sweet spot)
  alpha: 32               # Alpha scaling = 32 (per Schulman)
  dropout: 0.05           # Light dropout for stability
  bias: "none"            # No bias adaptation
  task_type: "CAUSAL_LM"
  
  # Target all linear layers (Schulman Sec. 2.2)
  target_modules:
    - "q_proj"      # Query projection
    - "k_proj"      # Key projection  
    - "v_proj"      # Value projection
    - "o_proj"      # Output projection
    - "gate_proj"   # Gate projection (MLP)
    - "up_proj"     # Up projection (MLP)
    - "down_proj"   # Down projection (MLP)

# Training Configuration
training:
  # Learning rate scaling (Schulman Sec. 3.1): LoRA LR ~10x FullFT LR
  learning_rate: 5e-5     # 10x higher than typical 5e-6 for full fine-tuning
  warmup_ratio: 0.05      # 5% warmup steps
  weight_decay: 0.01      # Regularization
  
  # Batch size (Schulman Sec. 4.2): moderate for stability
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 8  # Effective batch size = 32
  
  # Training duration
  num_train_epochs: 3
  max_steps: -1           # Use epochs instead
  eval_steps: 500
  save_steps: 1000
  logging_steps: 100
  
  # Optimization
  optim: "adamw_torch"
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8
  max_grad_norm: 1.0
  
  # Scheduler
  lr_scheduler_type: "cosine"
  
  # Mixed precision
  fp16: true
  dataloader_pin_memory: true

# Data Configuration  
data:
  max_length: 512         # Maximum sequence length
  truncation: true
  padding: "max_length"
  
  # Prompt template
  prompt_template: |
    Weather data for {location} on {datetime}:
    - Temperature (Â°C): {temperature}
    - Humidity (%): {humidity} 
    - Wind speed (km/h): {wind_speed}
    - Pressure (hPa): {pressure}
    - Precipitation probability: {precip_prob}
    
    Generate a forecast bulletin:
  
  # Target format
  target_template: |
    {forecast_text}

# Evaluation Configuration
evaluation:
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  evaluation_strategy: "steps"
  save_strategy: "steps"
  load_best_model_at_end: true

# Output Configuration
output:
  output_dir: "./models/weather-lora-sft"
  logging_dir: "./logs"
  run_name: "weather-lora-sft"
  report_to: ["tensorboard"]  # Can add "wandb" if configured
  
# Hardware Configuration
hardware:
  # Memory optimization
  gradient_checkpointing: true
  remove_unused_columns: false
  
  # Multi-GPU settings
  ddp_find_unused_parameters: false
  dataloader_num_workers: 4

# Seed for reproducibility
seed: 42