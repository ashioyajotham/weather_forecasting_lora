"""
LoRA Model Implementation for Weather Forecasting
=================================================

This module implements the LoRA fine-tuning approach following Schulman et al. (2025).
Key features:
- Frozen base weights with LoRA adapters
- All linear layers adaptation (attention + MLP)  
- Proper learning rate scaling (10x FullFT LR)
- Weather-specific prompt formatting

Following the "LoRA Without Regret" methodology.
"""

import torch
import torch.nn as nn
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer
)
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    TaskType
)
from datasets import Dataset
import logging
from typing import Dict, List, Optional, Union
import yaml
from pathlib import Path

logger = logging.getLogger(__name__)


class WeatherForecasterLoRA:
    """
    Weather forecasting model using LoRA fine-tuning.
    
    Implements the methodology from Schulman et al. (2025):
    - LoRA adapters on all linear layers
    - Frozen base model weights
    - Efficient adaptation for weather domain
    """
    
    def __init__(
        self,
        base_model_name: str = "microsoft/Mistral-7B-v0.1",
        lora_config: Optional[Dict] = None,
        quantization: bool = True,
        device_map: str = "auto"
    ):
        """
        Initialize WeatherForecasterLoRA model.
        
        Args:
            base_model_name: Hugging Face model name
            lora_config: LoRA configuration dictionary
            quantization: Whether to use 4-bit quantization
            device_map: Device mapping strategy
        """
        self.base_model_name = base_model_name
        self.device_map = device_map
        self.quantization = quantization
        
        # Default LoRA config following Schulman et al.
        self.lora_config = lora_config or {
            "r": 32,                    # Rank (Schulman recommends 16-64)
            "alpha": 32,               # Alpha scaling = 32
            "dropout": 0.05,           # Light dropout
            "bias": "none",            # No bias adaptation
            "task_type": TaskType.CAUSAL_LM,
            # Target all linear layers (Schulman Sec. 2.2)
            "target_modules": [
                "q_proj",      # Query projection
                "k_proj",      # Key projection
                "v_proj",      # Value projection
                "o_proj",      # Output projection
                "gate_proj",   # Gate projection (MLP)
                "up_proj",     # Up projection (MLP)
                "down_proj"    # Down projection (MLP)
            ]
        }
        
        self.model = None
        self.tokenizer = None
        self.peft_model = None
        
        logger.info(f"Initialized WeatherForecasterLoRA with {base_model_name}")
    
    def load_model(self):
        """Load base model and apply LoRA configuration."""
        logger.info("Loading base model and tokenizer...")
        
        # Load tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.base_model_name,
            trust_remote_code=True,
            use_fast=True
        )
        
        # Add padding token if not present
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
        
        # Configure quantization if enabled
        quantization_config = None
        if self.quantization:
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16
            )
        
        # Load base model
        self.model = AutoModelForCausalLM.from_pretrained(
            self.base_model_name,
            quantization_config=quantization_config,
            device_map=self.device_map,
            trust_remote_code=True,
            torch_dtype=torch.float16
        )
        
        # Prepare model for training if quantized
        if self.quantization:
            self.model = prepare_model_for_kbit_training(self.model)
        
        # Apply LoRA configuration
        lora_config = LoraConfig(**self.lora_config)
        self.peft_model = get_peft_model(self.model, lora_config)
        
        # Print trainable parameters
        self.peft_model.print_trainable_parameters()
        
        logger.info("Model loaded successfully with LoRA adapters")
        return self.peft_model
    
    def prepare_dataset(
        self,
        dataset: List[Dict],
        max_length: int = 512
    ) -> Dataset:
        """
        Prepare dataset for training.
        
        Args:
            dataset: List of training examples
            max_length: Maximum sequence length
            
        Returns:
            Hugging Face Dataset object
        """
        logger.info(f"Preparing dataset with {len(dataset)} examples")
        
        def tokenize_function(examples):
            """Tokenize input-target pairs."""
            # Combine prompt and response for causal LM training
            texts = []
            for input_text, target_text in zip(examples['input'], examples['target']):
                # Format as instruction-following conversation
                formatted_text = f"{input_text}\n{target_text}{self.tokenizer.eos_token}"
                texts.append(formatted_text)
            
            # Tokenize
            tokenized = self.tokenizer(
                texts,
                truncation=True,
                padding="max_length",
                max_length=max_length,
                return_tensors="pt"
            )
            
            # For causal LM, labels are the same as input_ids
            tokenized["labels"] = tokenized["input_ids"].clone()
            
            return tokenized
        
        # Convert to Hugging Face dataset
        hf_dataset = Dataset.from_list(dataset)
        
        # Tokenize dataset
        tokenized_dataset = hf_dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=hf_dataset.column_names,
            desc="Tokenizing dataset"
        )
        
        logger.info(f"Dataset prepared: {len(tokenized_dataset)} examples")
        return tokenized_dataset
    
    def create_training_arguments(
        self,
        output_dir: str = "./models/weather-lora-sft",
        config_path: Optional[str] = None
    ) -> TrainingArguments:
        """
        Create training arguments following Schulman et al. methodology.
        
        Args:
            output_dir: Directory to save model outputs
            config_path: Path to training configuration file
            
        Returns:
            TrainingArguments object
        """
        # Default training configuration (Schulman recommendations)
        default_config = {
            "output_dir": output_dir,
            "num_train_epochs": 3,
            "per_device_train_batch_size": 4,
            "per_device_eval_batch_size": 8,
            "gradient_accumulation_steps": 8,  # Effective batch size = 32
            "learning_rate": 5e-5,  # 10x higher than typical FullFT (Schulman Sec. 3.1)
            "weight_decay": 0.01,
            "warmup_ratio": 0.05,
            "lr_scheduler_type": "cosine",
            "max_grad_norm": 1.0,
            "fp16": True,
            "gradient_checkpointing": True,
            "dataloader_pin_memory": True,
            "eval_strategy": "steps",
            "eval_steps": 500,
            "save_strategy": "steps",
            "save_steps": 1000,
            "logging_steps": 100,
            "save_total_limit": 3,
            "load_best_model_at_end": True,
            "metric_for_best_model": "eval_loss",
            "greater_is_better": False,
            "report_to": ["tensorboard"],
            "run_name": "weather-lora-sft",
            "seed": 42,
            "remove_unused_columns": False,
            "ddp_find_unused_parameters": False
        }
        
        # Load config from file if provided
        if config_path and Path(config_path).exists():
            with open(config_path, 'r') as f:
                config = yaml.safe_load(f)
                if 'training' in config:
                    default_config.update(config['training'])
        
        # Create TrainingArguments
        training_args = TrainingArguments(**default_config)
        
        logger.info(f"Training arguments created for {training_args.num_train_epochs} epochs")
        logger.info(f"Learning rate: {training_args.learning_rate} (LoRA scaling)")
        logger.info(f"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}")
        
        return training_args
    
    def get_trainer(
        self,
        train_dataset: Dataset,
        eval_dataset: Optional[Dataset] = None,
        training_args: Optional[TrainingArguments] = None
    ) -> Trainer:
        """
        Create Hugging Face Trainer for LoRA fine-tuning.
        
        Args:
            train_dataset: Training dataset
            eval_dataset: Evaluation dataset
            training_args: Training arguments
            
        Returns:
            Trainer object
        """
        if self.peft_model is None:
            raise ValueError("Model not loaded. Call load_model() first.")
        
        if training_args is None:
            training_args = self.create_training_arguments()
        
        # Data collator for causal LM
        from transformers import DataCollatorForLanguageModeling
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False  # Causal LM, not masked LM
        )
        
        # Create trainer
        trainer = Trainer(
            model=self.peft_model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            data_collator=data_collator,
            tokenizer=self.tokenizer
        )
        
        logger.info("Trainer created successfully")
        return trainer
    
    def save_model(self, output_path: str):
        """
        Save the LoRA adapter weights.
        
        Args:
            output_path: Path to save the adapter
        """
        if self.peft_model is None:
            raise ValueError("No model to save. Train model first.")
        
        self.peft_model.save_pretrained(output_path)
        self.tokenizer.save_pretrained(output_path)
        
        logger.info(f"LoRA adapter saved to {output_path}")
    
    def load_adapter(self, adapter_path: str):
        """
        Load a trained LoRA adapter.
        
        Args:
            adapter_path: Path to the saved adapter
        """
        if self.model is None:
            self.load_model()
        
        from peft import PeftModel
        
        self.peft_model = PeftModel.from_pretrained(
            self.model,
            adapter_path,
            is_trainable=False
        )
        
        logger.info(f"LoRA adapter loaded from {adapter_path}")
        return self.peft_model
    
    def generate_forecast(
        self,
        prompt: str,
        max_new_tokens: int = 128,
        temperature: float = 0.7,
        do_sample: bool = True
    ) -> str:
        """
        Generate weather forecast from prompt.
        
        Args:
            prompt: Input weather data prompt
            max_new_tokens: Maximum tokens to generate
            temperature: Sampling temperature
            do_sample: Whether to use sampling
            
        Returns:
            Generated forecast text
        """
        if self.peft_model is None:
            raise ValueError("Model not loaded. Call load_model() or load_adapter() first.")
        
        # Tokenize input
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=512
        ).to(self.peft_model.device)
        
        # Generate
        with torch.no_grad():
            outputs = self.peft_model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                do_sample=do_sample,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
        
        # Decode and extract generated part
        full_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        generated_text = full_text[len(prompt):].strip()
        
        return generated_text


class LoRATrainer:
    """
    High-level trainer class for weather forecasting LoRA models.
    
    Handles the complete training pipeline with proper configurations
    following Schulman et al. methodology.
    """
    
    def __init__(
        self,
        model: WeatherForecasterLoRA,
        config_path: Optional[str] = None
    ):
        """
        Initialize LoRA trainer.
        
        Args:
            model: WeatherForecasterLoRA instance
            config_path: Path to configuration file
        """
        self.model = model
        self.config_path = config_path
        self.trainer = None
        
        logger.info("LoRATrainer initialized")
    
    def train(
        self,
        train_dataset: List[Dict],
        eval_dataset: Optional[List[Dict]] = None,
        output_dir: str = "./models/weather-lora-sft"
    ):
        """
        Complete training pipeline.
        
        Args:
            train_dataset: Training data
            eval_dataset: Evaluation data
            output_dir: Output directory
        """
        logger.info("Starting LoRA training pipeline...")
        
        # Load model if not already loaded
        if self.model.peft_model is None:
            self.model.load_model()
        
        # Prepare datasets
        train_hf_dataset = self.model.prepare_dataset(train_dataset)
        eval_hf_dataset = None
        if eval_dataset:
            eval_hf_dataset = self.model.prepare_dataset(eval_dataset)
        
        # Create training arguments
        training_args = self.model.create_training_arguments(
            output_dir=output_dir,
            config_path=self.config_path
        )
        
        # Create trainer
        self.trainer = self.model.get_trainer(
            train_dataset=train_hf_dataset,
            eval_dataset=eval_hf_dataset,
            training_args=training_args
        )
        
        # Start training
        logger.info("🚀 Starting LoRA fine-tuning...")
        self.trainer.train()
        
        # Save model
        self.model.save_model(output_dir)
        
        logger.info("✅ Training completed successfully!")
        logger.info(f"Model saved to: {output_dir}")
        
        return self.trainer.state.log_history


def main():
    """Example usage of WeatherForecasterLoRA."""
    # Initialize model
    model = WeatherForecasterLoRA(
        base_model_name="microsoft/Mistral-7B-v0.1",
        quantization=True
    )
    
    # Load model
    model.load_model()
    
    # Example training data
    example_data = [
        {
            "input": """Weather data for New York on 2025-09-30 12:00 UTC:
- Temperature (°C): 23.0, 24.0, 22.0, 21.0
- Humidity (%): 70, 75, 80, 82
- Wind speed (km/h): 12.0, 18.0, 20.0, 15.0
- Pressure (hPa): 1008.0, 1007.0, 1006.0, 1005.0
- Precipitation probability: 0.10, 0.20, 0.60, 0.70

Generate a forecast bulletin:""",
            "target": "Afternoon temperatures around 23-24°C with increasing humidity. Winds picking up to 20 km/h by early evening. Showers likely by evening with precipitation chances above 60%."
        }
    ]
    
    # Initialize trainer
    trainer = LoRATrainer(model)
    
    # Train (this would normally use a full dataset)
    # trainer.train(example_data)
    
    print("Example setup completed. Ready for training with full dataset.")


if __name__ == "__main__":
    main()