# Weather Forecasting LoRA - Test Suite

Comprehensive test suite for the Weather Forecasting LoRA project, following research-grade testing standards.

## ðŸ“‹ Test Structure

```
tests/
â”œâ”€â”€ __init__.py              # Test package configuration
â”œâ”€â”€ conftest.py              # Pytest fixtures and configuration
â”œâ”€â”€ test_data.py             # Data collection & preprocessing tests
â”œâ”€â”€ test_models.py           # LoRA model & training tests
â”œâ”€â”€ test_evaluation.py       # Evaluation framework tests
â”œâ”€â”€ test_inference.py        # Inference & deployment tests
â””â”€â”€ README.md               # This file
```

## ðŸš€ Running Tests

### Run All Tests
```bash
pytest tests/ -v
```

### Run Specific Test Files
```bash
# Data tests
pytest tests/test_data.py -v

# Model tests
pytest tests/test_models.py -v

# Evaluation tests
pytest tests/test_evaluation.py -v

# Inference tests
pytest tests/test_inference.py -v
```

### Run by Test Markers
```bash
# Run only unit tests
pytest tests/ -m unit

# Run only integration tests
pytest tests/ -m integration

# Skip slow tests
pytest tests/ -m "not slow"

# Run only GPU tests
pytest tests/ -m gpu

# Run without API-dependent tests
pytest tests/ -m "not api"
```

### Run with Coverage
```bash
# Generate coverage report
pytest tests/ --cov=src --cov-report=html

# View coverage report
open htmlcov/index.html  # macOS/Linux
start htmlcov/index.html  # Windows
```

## ðŸ·ï¸ Test Markers

| Marker | Description |
|--------|-------------|
| `unit` | Unit tests for individual components |
| `integration` | Integration tests for workflows |
| `slow` | Slow-running tests (skip with `-m "not slow"`) |
| `gpu` | Tests requiring GPU hardware |
| `api` | Tests requiring external API access |
| `performance` | Performance and benchmark tests |

## ðŸ“Š Test Coverage

The test suite covers:

### Data Module (`test_data.py`)
- âœ… Weather data collection from APIs
- âœ… Data preprocessing and formatting
- âœ… Numerical â†’ text conversion
- âœ… Dataset creation and splitting
- âœ… Data validation and error handling
- âœ… Caching mechanisms

### Models Module (`test_models.py`)
- âœ… LoRA model initialization
- âœ… Model configuration (Schulman et al. compliance)
- âœ… Training pipeline execution
- âœ… Forecast generation
- âœ… Adapter management
- âœ… Memory and performance optimization

### Evaluation Module (`test_evaluation.py`)
- âœ… BLEU/ROUGE metric calculation
- âœ… Meteorological accuracy metrics
- âœ… Rain prediction accuracy
- âœ… Temperature/wind MAE
- âœ… Calibration (Brier score)
- âœ… Evaluation report generation

### Inference Module (`test_inference.py`)
- âœ… Real-time inference
- âœ… Batch processing
- âœ… API integration
- âœ… Model versioning
- âœ… Error handling and recovery
- âœ… Performance and latency testing

## ðŸ§ª Test Categories

### Unit Tests (Fast)
Test individual functions and classes in isolation:
```bash
pytest tests/ -m unit --duration=10
```

### Integration Tests (Medium)
Test complete workflows and component interactions:
```bash
pytest tests/ -m integration
```

### Performance Tests (Slow)
Benchmark performance and resource usage:
```bash
pytest tests/ -m performance
```

## ðŸ“ Writing New Tests

### Test Template
```python
import pytest
from src.your_module import YourClass

@pytest.mark.unit
class TestYourClass:
    """Test YourClass functionality."""
    
    def test_initialization(self):
        """Test class initializes correctly."""
        obj = YourClass()
        assert obj is not None
    
    def test_method_behavior(self):
        """Test specific method behavior."""
        obj = YourClass()
        result = obj.your_method(input_data)
        assert result == expected_output
```

### Using Fixtures
```python
def test_with_fixtures(sample_weather_data, mock_lora_model):
    """Test using pytest fixtures."""
    # Fixtures are automatically injected
    assert sample_weather_data is not None
    assert mock_lora_model is not None
```

### Parametrized Tests
```python
@pytest.mark.parametrize("input,expected", [
    (10, 20),
    (20, 40),
    (30, 60),
])
def test_doubling(input, expected):
    """Test doubling function with multiple inputs."""
    assert double(input) == expected
```

## ðŸ”§ Continuous Integration

Tests are automatically run in CI/CD pipeline:

```yaml
# .github/workflows/tests.yml
- name: Run tests
  run: |
    pytest tests/ -m "not slow and not gpu"
    pytest tests/ --cov=src --cov-report=xml
```

## ðŸ“ˆ Test Metrics

Target metrics for test suite:
- **Coverage**: >80% code coverage
- **Speed**: Unit tests <5 seconds total
- **Reliability**: 100% pass rate on main branch
- **Maintenance**: Tests updated with code changes

## ðŸ› Debugging Tests

### Run with verbose output
```bash
pytest tests/ -vv
```

### Stop on first failure
```bash
pytest tests/ -x
```

### Run specific test
```bash
pytest tests/test_data.py::TestWeatherDataCollector::test_collector_initialization
```

### Show print statements
```bash
pytest tests/ -s
```

### Debug with pdb
```bash
pytest tests/ --pdb
```

## ðŸ“š Additional Resources

- [Pytest Documentation](https://docs.pytest.org/)
- [Contributing Guide](../CONTRIBUTING.md#testing-guidelines)
- [Project Documentation](../README.md)

## ðŸ™ Contributing Tests

When adding new features:
1. **Write tests first** (TDD approach)
2. **Follow existing patterns** in test files
3. **Add appropriate markers** (`@pytest.mark.unit`, etc.)
4. **Update this README** if adding new test categories
5. **Ensure tests pass** before submitting PR

---

For questions about testing, see [CONTRIBUTING.md](../CONTRIBUTING.md) or open an issue.